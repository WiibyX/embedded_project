{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "import keras.optimizers\n",
    "import random\n",
    "import scipy\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "raw",
   "id": "suspended-indicator",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "smaller-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgFileDir = r'C:\\Users\\WillyB\\Documents\\GitHub\\embedded_project\\Stanford40_JPEGImages\\JPEGImages'\n",
    "CImagesDir = r'C:\\Users\\WillyB\\Documents\\GitHub\\embedded_project\\Stanford40_JPEGImages\\CImages'\n",
    "parentImgFileDir = r'C:\\Users\\WillyB\\Documents\\GitHub\\embedded_project\\Stanford40_JPEGImages'\n",
    "xmlFileDir = r'C:\\Users\\WillyB\\Documents\\GitHub\\embedded_project\\Stanford40_XMLAnnotations\\XMLAnnotations'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "functional-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractClassLabels(xmlFileDir):\n",
    "    os.chdir(xmlFileDir)\n",
    "    xmlFileNames = [f for f in listdir(xmlFileDir) if isfile(join(xmlFileDir, f))]\n",
    "    #xmlFileNames = ['3m1.xml','3m2.xml','3m3.xml','3m4.xml']\n",
    "    classLabels = []\n",
    "    imageNames = []\n",
    "    count = 1\n",
    "    for file in xmlFileNames:\n",
    "        root = ET.parse(file).getroot()\n",
    "        filename = root.find('filename').text\n",
    "        action = root.find('object/action').text\n",
    "        classLabels.append(action)\n",
    "        imageNames.append(filename)\n",
    "        print(str(count)+'/'+str(len(xmlFileNames)), end='\\r')\n",
    "        count +=1\n",
    "    return np.array(classLabels), np.array(imageNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "thirty-trust",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9532/9532\r"
     ]
    }
   ],
   "source": [
    "classList, allImageNames = extractClassLabels(xmlFileDir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "proprietary-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "(classUnique,counts) = np.unique(classList, return_counts=True)\n",
    "\n",
    "(_,inverse) = np.unique(classList, return_inverse=True)\n",
    "\n",
    "y = to_categorical(inverse,len(classUnique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "removed-assignment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9532 files belonging to 40 classes.\n",
      "Using 7626 files for training.\n",
      "Found 9532 files belonging to 40 classes.\n",
      "Using 1906 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "img_height = 90\n",
    "img_width = 90\n",
    "actionImagesDir = r'C:\\Users\\WillyB\\Documents\\GitHub\\embedded_project\\Stanford40_JPEGImages\\ActionImages'\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    actionImagesDir,\n",
    "    validation_split=0.2,\n",
    "    label_mode = 'categorical',\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    actionImagesDir,\n",
    "    validation_split=0.2,\n",
    "    label_mode = 'categorical',\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "looking-carol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x000001A831933438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x000001A831933438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "absolute-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "#train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "built-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model(nrOfClasses):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(90 ,90, 3)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(nrOfClasses, activation='softmax'))\n",
    "    opt = keras.optimizers.Adam(0.0001)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tuned_model = best_model(len(classUnique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "published-actress",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(nrOfClasses, hyperparameters):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(hyperparameters[\"nrOfFiltersInConvLayers\"][0], (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(90 ,90, 3)))\n",
    "    \n",
    "    for nrOfFilters in hyperparameters[\"nrOfFiltersInConvLayers\"][1:]:\n",
    "        model.add(Conv2D(nrOfFilters, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Dropout(hyperparameters[\"dropout\"]))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    for nrOfNeurons in hyperparameters[\"nrOfNeuronsInHiddenLayers\"]:\n",
    "        model.add(Dense(nrOfNeurons, activation='relu', kernel_initializer='he_uniform'))\n",
    "        \n",
    "    model.add(Dense(nrOfClasses, activation='softmax'))\n",
    "    # compile modelÂ¨\n",
    "    opt = keras.optimizers.Adam(learning_rate=hyperparameters[\"learningRate\"])\n",
    "    #opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "lyric-springfield",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nrOfFiltersInConvLayers': [128, 16, 16], 'nrOfNeuronsInHiddenLayers': [128, 128], 'dropout': 0.5, 'learningRate': 0.001}\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A83FF4A708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A83FF4A708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "239/239 [==============================] - 16s 67ms/step - loss: 3.6948 - accuracy: 0.0316\n",
      "Epoch 2/20\n",
      "239/239 [==============================] - 12s 52ms/step - loss: 3.6014 - accuracy: 0.06330s - l\n",
      "Epoch 3/20\n",
      "239/239 [==============================] - 12s 52ms/step - loss: 3.4711 - accuracy: 0.0943\n",
      "Epoch 4/20\n",
      "239/239 [==============================] - 13s 53ms/step - loss: 3.3480 - accuracy: 0.1218\n",
      "Epoch 5/20\n",
      "239/239 [==============================] - 13s 53ms/step - loss: 3.2247 - accuracy: 0.1454\n",
      "Epoch 6/20\n",
      "239/239 [==============================] - 12s 52ms/step - loss: 3.1020 - accuracy: 0.1748\n",
      "Epoch 7/20\n",
      "239/239 [==============================] - 13s 53ms/step - loss: 2.9541 - accuracy: 0.2068\n",
      "Epoch 8/20\n",
      "239/239 [==============================] - 14s 59ms/step - loss: 2.8141 - accuracy: 0.2377\n",
      "Epoch 9/20\n",
      "239/239 [==============================] - 13s 56ms/step - loss: 2.6464 - accuracy: 0.2787\n",
      "Epoch 10/20\n",
      "239/239 [==============================] - 13s 53ms/step - loss: 2.5196 - accuracy: 0.3020\n",
      "Epoch 11/20\n",
      "239/239 [==============================] - 13s 56ms/step - loss: 2.3644 - accuracy: 0.3478\n",
      "Epoch 12/20\n",
      "239/239 [==============================] - 13s 55ms/step - loss: 2.2299 - accuracy: 0.3807\n",
      "Epoch 13/20\n",
      "239/239 [==============================] - 13s 54ms/step - loss: 2.0637 - accuracy: 0.4166\n",
      "Epoch 14/20\n",
      "239/239 [==============================] - 13s 55ms/step - loss: 1.9639 - accuracy: 0.4344\n",
      "Epoch 15/20\n",
      "239/239 [==============================] - 13s 55ms/step - loss: 1.8433 - accuracy: 0.4689\n",
      "Epoch 16/20\n",
      "239/239 [==============================] - 13s 56ms/step - loss: 1.7023 - accuracy: 0.50940s - loss: 1.7055 - accura\n",
      "Epoch 17/20\n",
      "239/239 [==============================] - 13s 53ms/step - loss: 1.5991 - accuracy: 0.5350\n",
      "Epoch 18/20\n",
      "239/239 [==============================] - 13s 54ms/step - loss: 1.5024 - accuracy: 0.5556\n",
      "Epoch 19/20\n",
      "239/239 [==============================] - 13s 53ms/step - loss: 1.4270 - accuracy: 0.5817\n",
      "Epoch 20/20\n",
      "239/239 [==============================] - 13s 53ms/step - loss: 1.3623 - accuracy: 0.5985\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A83FF4A8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A83FF4A8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "60/60 [==============================] - 3s 42ms/step - loss: 588.1947 - accuracy: 0.0855\n",
      "0.08551941066980362\n",
      "{'nrOfFiltersInConvLayers': [128, 16, 16], 'nrOfNeuronsInHiddenLayers': [128, 128], 'dropout': 0.5, 'learningRate': 0.001}\n",
      "{'nrOfFiltersInConvLayers': [16, 16, 16, 16], 'nrOfNeuronsInHiddenLayers': [128, 32, 128, 32], 'dropout': 0.8, 'learningRate': 1e-05}\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A823729798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A823729798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "239/239 [==============================] - 9s 38ms/step - loss: 16.7015 - accuracy: 0.0261\n",
      "Epoch 2/20\n",
      " 92/239 [==========>...................] - ETA: 6s - loss: 11.1473 - accuracy: 0.0228- ETA: 8s - los"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-a1c995c848db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefine_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassUnique\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nrOfConvLayers = [2,3,4]\n",
    "nrOfFilters = [16,32,64,128]\n",
    "nrOfHiddenLayers = [2,3,4]\n",
    "nrOfHiddenLayerNeurons = [32,64,128,256,512]\n",
    "dropout = [0.2,0.5,0.8]\n",
    "learningRate = [0.00001,0.0001,0.001]\n",
    "\n",
    "tests = 100\n",
    "\n",
    "bestAccuracy = 0\n",
    "bestHyperparameters = None\n",
    "\n",
    "for test in range(tests):\n",
    "    hyperparameters = {\n",
    "        'nrOfFiltersInConvLayers': [],\n",
    "        'nrOfNeuronsInHiddenLayers':[],\n",
    "        'dropout': random.choice(dropout),\n",
    "        'learningRate':random.choice(learningRate)\n",
    "    }\n",
    "    \n",
    "    for layer in range(random.choice(nrOfConvLayers)):\n",
    "        hyperparameters['nrOfFiltersInConvLayers'].append(random.choice(nrOfFilters))\n",
    "    \n",
    "    \n",
    "    for layer in range(random.choice(nrOfHiddenLayers)):\n",
    "        hyperparameters['nrOfNeuronsInHiddenLayers'].append(random.choice(nrOfHiddenLayerNeurons))\n",
    "                       \n",
    "    print(hyperparameters)\n",
    "    \n",
    "    model = define_model(len(classUnique),hyperparameters)\n",
    "    model.fit(train_ds, epochs=20)\n",
    "    result = model.evaluate(test_ds, batch_size=32)\n",
    "    accuracy = result[1]\n",
    "    \n",
    "    if accuracy > bestAccuracy:\n",
    "        bestAccuracy = accuracy\n",
    "        bestHyperparameters = hyperparameters\n",
    "    \n",
    "    print(bestAccuracy)\n",
    "    print(bestHyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "consistent-canadian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A840140DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A840140DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "239/239 [==============================] - 11s 47ms/step - loss: 3.6912 - accuracy: 0.0382\n",
      "Epoch 2/20\n",
      "239/239 [==============================] - 11s 47ms/step - loss: 3.5614 - accuracy: 0.0809\n",
      "Epoch 3/20\n",
      "239/239 [==============================] - 11s 46ms/step - loss: 3.3291 - accuracy: 0.13301s - los\n",
      "Epoch 4/20\n",
      "239/239 [==============================] - 11s 46ms/step - loss: 3.1114 - accuracy: 0.18661s - loss: 3\n",
      "Epoch 5/20\n",
      "239/239 [==============================] - 11s 46ms/step - loss: 2.9085 - accuracy: 0.2318\n",
      "Epoch 6/20\n",
      "239/239 [==============================] - 11s 46ms/step - loss: 2.7489 - accuracy: 0.27504s - loss: 2 - ETA\n",
      "Epoch 7/20\n",
      "239/239 [==============================] - 11s 47ms/step - loss: 2.5973 - accuracy: 0.3058\n",
      "Epoch 8/20\n",
      "239/239 [==============================] - 13s 53ms/step - loss: 2.4242 - accuracy: 0.34974s - loss: - ETA: 4s - loss: 2.478\n",
      "Epoch 9/20\n",
      "239/239 [==============================] - 13s 53ms/step - loss: 2.2647 - accuracy: 0.3828\n",
      "Epoch 10/20\n",
      "239/239 [==============================] - 12s 50ms/step - loss: 2.1018 - accuracy: 0.4242\n",
      "Epoch 11/20\n",
      "239/239 [==============================] - 12s 49ms/step - loss: 1.9094 - accuracy: 0.47550s - loss: 1.9179 - accu\n",
      "Epoch 12/20\n",
      "239/239 [==============================] - 11s 47ms/step - loss: 1.7354 - accuracy: 0.5172\n",
      "Epoch 13/20\n",
      "239/239 [==============================] - 11s 47ms/step - loss: 1.5422 - accuracy: 0.5720\n",
      "Epoch 14/20\n",
      "239/239 [==============================] - 12s 51ms/step - loss: 1.3534 - accuracy: 0.6161\n",
      "Epoch 15/20\n",
      "239/239 [==============================] - 11s 48ms/step - loss: 1.1461 - accuracy: 0.6757\n",
      "Epoch 16/20\n",
      "239/239 [==============================] - 11s 48ms/step - loss: 0.9780 - accuracy: 0.7135\n",
      "Epoch 17/20\n",
      "239/239 [==============================] - 11s 48ms/step - loss: 0.7973 - accuracy: 0.7709\n",
      "Epoch 18/20\n",
      "239/239 [==============================] - 11s 48ms/step - loss: 0.6489 - accuracy: 0.8116\n",
      "Epoch 19/20\n",
      "239/239 [==============================] - 12s 49ms/step - loss: 0.5196 - accuracy: 0.8461\n",
      "Epoch 20/20\n",
      "239/239 [==============================] - 11s 48ms/step - loss: 0.4224 - accuracy: 0.8727\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A820457678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A820457678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "60/60 [==============================] - 2s 31ms/step - loss: 1040.2345 - accuracy: 0.2104: 0s - loss: 1022.6024 - accuracy: - ETA: 0s - loss: 1044.7343 - accuracy: 0\n"
     ]
    }
   ],
   "source": [
    "tuned_model.fit(train_ds, epochs=20)\n",
    "result = tuned_model.evaluate(test_ds, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "younger-ministry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1040.2344970703125, 0.21038824319839478]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "practical-stanley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it exist\n",
      "WARNING:tensorflow:From C:\\Users\\WillyB\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\WillyB\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001A99DF89558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001A99DF89558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: saved_model/my_model\\assets\n"
     ]
    }
   ],
   "source": [
    "parentModelDir = r'C:\\Users\\WillyB\\Documents\\GitHub\\embedded_project\\code'\n",
    "os.chdir(parentModelDir)\n",
    "savedModelDir = os.path.join(parentModelDir, \"saved_model\")\n",
    "if os.path.exists(savedModelDir):\n",
    "    print(\"it exist\")\n",
    "else:\n",
    "    print(\"does not exist, create folder\")\n",
    "    os.mkdir(savedModelDir)\n",
    "\n",
    "tuned_model.save('saved_model/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "polar-still",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to tensorflow lite model\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(savedModelDir+'/my_model') # path to the SavedModel directory\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "os.makedirs(os.path.join(os.getcwd(),'saved_tflite_models'), exist_ok = True)\n",
    "# Save the model.\n",
    "with open('./saved_tflite_models/model_1.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "solid-employment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tensorflow lite model\n",
    "interpreter = tf.lite.Interpreter(model_path='./saved_tflite_models/model_1.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "noted-planning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'conv2d_7_input',\n",
       "  'index': 0,\n",
       "  'shape': array([ 1, 90, 90,  3]),\n",
       "  'shape_signature': array([-1, 90, 90,  3]),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "hourly-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_batches = tfds.as_numpy(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "analyzed-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_images = 0\n",
    "for batch in ds_batches:\n",
    "    for image in batch[1]:\n",
    "        count_images +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "white-hebrew",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1906"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "relevant-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTest = np.zeros((count_images, 90,90,3)).astype('float32')\n",
    "yTest = np.zeros((count_images,40)).astype('float32')\n",
    "\n",
    "batch_number = 0\n",
    "for batch in ds_batches:\n",
    "    for image_number in range(len(batch[0])):\n",
    "        xTest[batch_number*32+image_number] = batch[0][image_number]\n",
    "        yTest[batch_number*32+image_number] = batch[1][image_number]\n",
    "    batch_number +=1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "quantitative-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predict = 0\n",
    "for image_number in range(len(xTest)): \n",
    "    interpreter.set_tensor(input_details[0]['index'], xTest[image_number][np.newaxis, ...])\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    if (output_data[0] == yTest[image_number]).all():\n",
    "        correct_predict +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "marked-oliver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18363064008394545"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = correct_predict/count_images\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
