{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "affecting-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "import keras.optimizers\n",
    "import random\n",
    "import scipy\n",
    "import math\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "smaller-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgFileDir = r'C:\\Users\\WillyB\\Documents\\GitHub\\embedded_project\\Stanford40_JPEGImages\\JPEGImages'\n",
    "CImagesDir = r'C:\\Users\\WillyB\\Documents\\GitHub\\embedded_project\\Stanford40_JPEGImages\\CImages'\n",
    "parentImgFileDir = r'C:\\Users\\WillyB\\Documents\\GitHub\\embedded_project\\Stanford40_JPEGImages'\n",
    "xmlFileDir = r'C:\\Users\\WillyB\\Documents\\GitHub\\embedded_project\\Stanford40_XMLAnnotations\\XMLAnnotations'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "functional-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractClassLabels(xmlFileDir):\n",
    "    os.chdir(xmlFileDir)\n",
    "    xmlFileNames = [f for f in listdir(xmlFileDir) if isfile(join(xmlFileDir, f))]\n",
    "    #xmlFileNames = ['3m1.xml','3m2.xml','3m3.xml','3m4.xml']\n",
    "    classLabels = []\n",
    "    imageNames = []\n",
    "    count = 1\n",
    "    for file in xmlFileNames:\n",
    "        root = ET.parse(file).getroot()\n",
    "        filename = root.find('filename').text\n",
    "        action = root.find('object/action').text\n",
    "        classLabels.append(action)\n",
    "        imageNames.append(filename)\n",
    "        print(str(count)+'/'+str(len(xmlFileNames)), end='\\r')\n",
    "        count +=1\n",
    "    return np.array(classLabels), np.array(imageNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "thirty-trust",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9532/9532\r"
     ]
    }
   ],
   "source": [
    "classList, allImageNames = extractClassLabels(xmlFileDir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "proprietary-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "(classUnique,counts) = np.unique(classList, return_counts=True)\n",
    "\n",
    "(_,inverse) = np.unique(classList, return_inverse=True)\n",
    "\n",
    "y = to_categorical(inverse,len(classUnique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "removed-assignment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9532 files belonging to 40 classes.\n",
      "Using 7626 files for training.\n",
      "Found 9532 files belonging to 40 classes.\n",
      "Using 1906 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "img_height = 90\n",
    "img_width = 90\n",
    "actionImagesDir = r'C:\\Users\\WillyB\\Documents\\GitHub\\embedded_project\\Stanford40_JPEGImages\\ActionImages'\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    actionImagesDir,\n",
    "    validation_split=0.2,\n",
    "    label_mode = 'categorical',\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    actionImagesDir,\n",
    "    validation_split=0.2,\n",
    "    label_mode = 'categorical',\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "looking-carol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x0000029B29AB8048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x0000029B29AB8048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "absolute-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "#train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "explicit-birmingham",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 88, 88, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 44, 44, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 44, 44, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 35, 35, 64)        204864    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 35, 35, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 6, 6, 64)          3686464   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               295424    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 40)                5160      \n",
      "=================================================================\n",
      "Total params: 4,357,032\n",
      "Trainable params: 4,357,032\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def define_model(nrOfClasses):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(90 ,90, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Dropout(0.50))\n",
    "\n",
    "    model.add(Conv2D(64, (10, 10), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.50))\n",
    "    \n",
    "    model.add(Conv2D(64, (30, 30), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(nrOfClasses, activation='softmax'))\n",
    "    # compile modelÂ¨\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    #opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "#model = define_model(len(classUnique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "published-actress",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(nrOfClasses, hyperparameters):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(hyperparameters[\"nrOfFiltersInConvLayers\"][0], (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(90 ,90, 3)))\n",
    "    \n",
    "    for nrOfFilters in hyperparameters[\"nrOfFiltersInConvLayers\"][1:]:\n",
    "        model.add(Conv2D(nrOfFilters, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Dropout(hyperparameters[\"dropout\"]))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    for nrOfNeurons in hyperparameters[\"nrOfNeuronsInHiddenLayers\"]:\n",
    "        model.add(Dense(nrOfNeurons, activation='relu', kernel_initializer='he_uniform'))\n",
    "        \n",
    "    model.add(Dense(nrOfClasses, activation='softmax'))\n",
    "    # compile modelÂ¨\n",
    "    opt = keras.optimizers.Adam(learning_rate=hyperparameters[\"learningRate\"])\n",
    "    #opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lyric-springfield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nrOfFiltersInConvLayers': [32, 128, 128], 'nrOfNeuronsInHiddenLayers': [128, 256, 256], 'dropOut': 0.5, 'learningRate': 1e-05}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'classUnique' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-60e8695dc81d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefine_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassUnique\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classUnique' is not defined"
     ]
    }
   ],
   "source": [
    "nrOfConvLayers = [2,3,4]\n",
    "nrOfFilters = [16,32,64,128]\n",
    "nrOfHiddenLayers = [2,3,4]\n",
    "nrOfHiddenLayerNeurons = [32,64,128,256,512]\n",
    "dropout = [0.2,0.5,0.8]\n",
    "learningRate = [0.00001,0.0001,0.001]\n",
    "\n",
    "tests = 2\n",
    "\n",
    "bestAccuracy = 0\n",
    "bestHyperparameters = None\n",
    "\n",
    "for test in range(tests):\n",
    "    hyperparameters = {\n",
    "        'nrOfFiltersInConvLayers': [],\n",
    "        'nrOfNeuronsInHiddenLayers':[],\n",
    "        'dropOut': random.choice(dropout),\n",
    "        'learningRate':random.choice(learningRate)\n",
    "    }\n",
    "    \n",
    "    for layer in range(random.choice(nrOfConvLayers)):\n",
    "        hyperparameters['nrOfFiltersInConvLayers'].append(random.choice(nrOfFilters))\n",
    "    \n",
    "    \n",
    "    for layer in range(random.choice(nrOfHiddenLayers)):\n",
    "        hyperparameters['nrOfNeuronsInHiddenLayers'].append(random.choice(nrOfHiddenLayerNeurons))\n",
    "                       \n",
    "    print(hyperparameters)\n",
    "    \n",
    "    model = define_model(len(classUnique),hyperparameters)\n",
    "    model.fit(train_ds, epochs=20, verbose=1)\n",
    "    result = model.evaluate(test_ds, batch_size=32, verbose =1)\n",
    "    accuracy = result[1]\n",
    "    \n",
    "    if accuracy > bestAccuracy:\n",
    "        bestAccuracy = accuracy\n",
    "        bestHyperparameters = hyperparameters\n",
    "    \n",
    "    print(bestAccuracy)\n",
    "    print(bestHyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-evidence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000029C9C4EEE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000029C9C4EEE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "239/239 [==============================] - 12s 52ms/step - loss: 3.7596 - accuracy: 0.0285\n",
      "Epoch 2/20\n",
      "239/239 [==============================] - 14s 58ms/step - loss: 3.6859 - accuracy: 0.0294\n",
      "Epoch 3/20\n",
      "239/239 [==============================] - 13s 56ms/step - loss: 3.6835 - accuracy: 0.0275\n",
      "Epoch 4/20\n",
      "239/239 [==============================] - 13s 53ms/step - loss: 3.6812 - accuracy: 0.0307\n",
      "Epoch 5/20\n",
      "239/239 [==============================] - 15s 62ms/step - loss: 3.6793 - accuracy: 0.0319\n",
      "Epoch 6/20\n",
      "239/239 [==============================] - 14s 57ms/step - loss: 3.6780 - accuracy: 0.0320\n",
      "Epoch 7/20\n",
      "239/239 [==============================] - 13s 53ms/step - loss: 3.6773 - accuracy: 0.0319\n",
      "Epoch 8/20\n",
      "239/239 [==============================] - 12s 52ms/step - loss: 3.6768 - accuracy: 0.0320\n",
      "Epoch 9/20\n",
      "239/239 [==============================] - ETA: 0s - loss: 3.6765 - accuracy: 0.03 - 13s 53ms/step - loss: 3.6765 - accuracy: 0.0319\n",
      "Epoch 10/20\n",
      "239/239 [==============================] - 13s 53ms/step - loss: 3.6763 - accuracy: 0.0319\n",
      "Epoch 11/20\n",
      "239/239 [==============================] - 13s 53ms/step - loss: 3.6761 - accuracy: 0.0319\n",
      "Epoch 12/20\n",
      "239/239 [==============================] - 12s 52ms/step - loss: 3.6761 - accuracy: 0.0319\n",
      "Epoch 13/20\n",
      "239/239 [==============================] - 13s 53ms/step - loss: 3.6760 - accuracy: 0.0319\n",
      "Epoch 14/20\n",
      "239/239 [==============================] - 13s 56ms/step - loss: 3.6759 - accuracy: 0.03191s - loss: 3 - ETA: 0s - loss: 3.6763 - accuracy\n",
      "Epoch 15/20\n",
      "239/239 [==============================] - 13s 53ms/step - loss: 3.6759 - accuracy: 0.0307\n",
      "Epoch 16/20\n",
      "124/239 [==============>...............] - ETA: 6s - loss: 3.6779 - accuracy: 0.0297 ETA: 7s - loss: 3.6783 - ac"
     ]
    }
   ],
   "source": [
    "model.fit(train_ds, epochs = 20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "detected-magic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000029B29DACEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000029B29DACEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "60/60 [==============================] - 2s 36ms/step - loss: 388.2030 - accuracy: 0.1264\n",
      "[388.2030334472656, 0.1264428049325943]\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(test_ds, batch_size=32, verbose =1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "scientific-kitchen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.18.5\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-supervision",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
